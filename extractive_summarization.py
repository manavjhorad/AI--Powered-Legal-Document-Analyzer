# -*- coding: utf-8 -*-
"""Extractive_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BFU14olRq9AU2GDmbLeCsC0s8wfhcaC9

#TextRank Algorithm
"""

!pip install pandas networkx nltk rouge-score openpyxl
!pip install spicy

import pandas as pd
import numpy as np
import nltk
import networkx as nx
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from rouge_score import rouge_scorer
import string
import itertools

nltk.download('punkt_tab')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def preprocess_sentence(sentence):
    tokens = word_tokenize(sentence.lower())
    tokens = [w for w in tokens if w not in stop_words and w not in string.punctuation]
    return tokens

def sentence_similarity(sent1, sent2):
    words1 = preprocess_sentence(sent1)
    words2 = preprocess_sentence(sent2)
    all_words = list(set(words1 + words2))

    vec1 = [0] * len(all_words)
    vec2 = [0] * len(all_words)

    for w in words1:
        vec1[all_words.index(w)] += 1

    for w in words2:
        vec2[all_words.index(w)] += 1

    # Cosine similarity
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude = np.sqrt(sum(a * a for a in vec1)) * np.sqrt(sum(b * b for b in vec2))

    return dot_product / magnitude if magnitude != 0 else 0

def build_similarity_matrix(sentences):
    matrix = np.zeros((len(sentences), len(sentences)))
    for i, j in itertools.product(range(len(sentences)), repeat=2):
        if i != j:
            matrix[i][j] = sentence_similarity(sentences[i], sentences[j])
    return matrix

def textrank_summary(text, num_sentences=2):
    sentences = sent_tokenize(text)
    if len(sentences) <= num_sentences:
        return text  # Return original text if it's too short

    sim_matrix = build_similarity_matrix(sentences)
    graph = nx.from_numpy_array(sim_matrix)
    scores = nx.pagerank(graph)

    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    selected = [s for _, s in ranked[:num_sentences]]
    return ' '.join(selected)

train_df = pd.read_excel('train_data.xlsx')
test_df = pd.read_excel('test_data.xlsx')

generated_summaries = [textrank_summary(text) for text in test_df['Text']]
reference_summaries = test_df['Summary'].tolist() # Change 'summary' to 'Summary'

generated_summaries[0]

reference_summaries[0]

def evaluate_rouge(reference_summaries, generated_summaries):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

    for ref, gen in zip(reference_summaries, generated_summaries):
        result = scorer.score(ref, gen)
        for k in scores.keys():
            scores[k].append(result[k].fmeasure)

    # Average scores
    avg_scores = {k: np.mean(v) for k, v in scores.items()}
    return avg_scores


rouge_results = evaluate_rouge(reference_summaries, generated_summaries)

print("ROUGE Scores:")
for k, v in rouge_results.items():
    print(f"{k}: {v:.4f}")

"""#LexRank Algorithm

"""

!pip install lexrank
!pip install pandas nltk networkx rouge-score openpyxl

import pandas as pd
from lexrank import LexRank
from lexrank.mappings.stopwords import STOPWORDS
from path import Path
from nltk.tokenize import sent_tokenize
from rouge_score import rouge_scorer
import numpy as np
import nltk
from nltk.corpus import stopwords


nltk.download('punkt_tab')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

# Load training and test data
train_df = pd.read_excel("train_data.xlsx")
test_df = pd.read_excel("test_data.xlsx")

# Prepare training documents for LexRank
train_docs = [sent_tokenize(text) for text in train_df["Text"]]

# Initialize LexRank
lxr = LexRank(train_docs, stopwords=STOPWORDS['en'])

# Apply LexRank to generate summaries for test data
generated_summaries = []
for text in test_df["Text"]:
    sentences = sent_tokenize(text)
    if len(sentences) <= 2:
        generated_summaries.append(text)
    else:
        summary = lxr.get_summary(sentences, summary_size=2, threshold=0.1)
        generated_summaries.append(" ".join(summary))

# Evaluate using ROUGE
reference_summaries = test_df["Summary"].tolist()
scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)

rouge_scores = {"rouge1": [], "rouge2": [], "rougeL": []}
for ref, gen in zip(reference_summaries, generated_summaries):
    score = scorer.score(ref, gen)
    for key in rouge_scores:
        rouge_scores[key].append(score[key].fmeasure)

# Calculate average scores
average_scores = {key: np.mean(vals) for key, vals in rouge_scores.items()}
average_scores

import pandas as pd
import numpy as np
import nltk
import networkx as nx
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from rouge_score import rouge_scorer
import string
import itertools

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

# --------- Preprocessing ---------
def preprocess_sentences(sentences):
    cleaned = []
    for s in sentences:
        tokens = word_tokenize(s.lower())
        tokens = [w for w in tokens if w not in stop_words and w not in string.punctuation]
        cleaned.append(' '.join(tokens))
    return cleaned


# --------- Build LexRank Similarity Matrix ---------
def build_lexrank_matrix(sentences, threshold=0.1):
    cleaned = preprocess_sentences(sentences)
    tfidf = TfidfVectorizer()
    tfidf_matrix = tfidf.fit_transform(cleaned)

    # Compute cosine similarity matrix
    cosine_sim = (tfidf_matrix * tfidf_matrix.T).toarray()

    # Apply threshold to build unweighted graph
    n = len(sentences)
    sim_matrix = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            if i != j and cosine_sim[i][j] > threshold:
                sim_matrix[i][j] = cosine_sim[i][j]
    return sim_matrix


# --------- LexRank Summarizer ---------
def lexrank_summary(text, num_sentences=2, threshold=0.1):
    sentences = sent_tokenize(text)
    if len(sentences) <= num_sentences:
        return text

    sim_matrix = build_lexrank_matrix(sentences, threshold)
    graph = nx.from_numpy_array(sim_matrix)
    scores = nx.pagerank(graph)

    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    selected = [s for _, s in ranked[:num_sentences]]
    return ' '.join(selected)


# --------- Load Excel Data ---------
train_df = pd.read_excel('train_data.xlsx')  # Not used in unsupervised LexRank
test_df = pd.read_excel('test_data.xlsx')


# --------- Evaluate Using ROUGE ---------
def evaluate_rouge(reference_summaries, generated_summaries):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

    for ref, gen in zip(reference_summaries, generated_summaries):
        result = scorer.score(ref, gen)
        for k in scores.keys():
            scores[k].append(result[k].fmeasure)

    avg_scores = {k: np.mean(v) for k, v in scores.items()}
    return avg_scores


# --------- Run Summarization and Evaluation ---------
generated_summaries = [lexrank_summary(text) for text in test_df['Text']]
reference_summaries = test_df['Summary'].tolist()

rouge_results = evaluate_rouge(reference_summaries, generated_summaries)

print("LexRank ROUGE Scores:")
for k, v in rouge_results.items():
    print(f"{k}: {v:.4f}")

!pip install transformers sentencepiece rouge-score pandas openpyxl
import pandas as pd
import numpy as np
import nltk
import networkx as nx
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from rouge_score import rouge_scorer
import string
import torch
from transformers import PegasusTokenizer, PegasusForConditionalGeneration

nltk.download('punkt_tab')
nltk.download('stopwords')

# Step 2: Load test data
test_df = pd.read_excel("test_data.xlsx",)
texts = test_df["Text"].tolist()
references = test_df["Summary"].tolist()

# Step 3: Define TextRank functions
stop_words = set(stopwords.words('english'))

def preprocess_sentence(sentence):
    words = word_tokenize(sentence.lower())
    return [w for w in words if w not in stop_words and w not in string.punctuation]

def textrank_rank_sentences(text):
    sentences = sent_tokenize(text)
    if len(sentences) <= 1:
        return sentences
    preprocessed = [preprocess_sentence(s) for s in sentences]

    similarity_matrix = np.zeros((len(sentences), len(sentences)))
    for i in range(len(sentences)):
        for j in range(len(sentences)):
            if i == j:
                continue
            words_i = set(preprocessed[i])
            words_j = set(preprocessed[j])
            if not words_i or not words_j:
                continue
            common_words = words_i.intersection(words_j)
            if common_words:
                similarity_matrix[i][j] = len(common_words) / (np.log(len(words_i)+1) + np.log(len(words_j)+1))

    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    return [s for _, s in ranked_sentences]

# Step 4: Load Legal-Pegasus
model_name = "nsi319/legal-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

def generate_pegasus_summary(text, max_length=128, min_length=30):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="longest", max_length=512).to(device)
    summary_ids = model.generate(inputs["input_ids"], max_length=max_length, min_length=min_length, length_penalty=1.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Step 5: Hybrid Summarization (TextRank + Pegasus)
generated_summaries = []
for text in texts:
    ranked_sentences = textrank_rank_sentences(text)
    top_sentences = " ".join(ranked_sentences[:10])  # select top 10 sentences
    summary = generate_pegasus_summary(top_sentences)
    generated_summaries.append(summary)

# Step 6: ROUGE Evaluation
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
rouge_scores = {"rouge1": [], "rouge2": [], "rougeL": []}

for ref, gen in zip(references, generated_summaries):
    score = scorer.score(ref, gen)
    for key in rouge_scores:
        rouge_scores[key].append(score[key].fmeasure)

average_scores = {key: np.mean(vals) for key, vals in rouge_scores.items()}
print("ROUGE Scores:")
for key, val in average_scores.items():
    print(f"{key}: {val:.4f}")

generated_summaries[0]

references[0]

