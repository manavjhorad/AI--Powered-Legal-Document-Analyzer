# -*- coding: utf-8 -*-
"""Hyrid model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15wsL0cIQVZXLb5tL3Yl9SJRgrdzNPRAw
"""

!pip install transformers sentencepiece rouge-score pandas openpyxl

import pandas as pd
import numpy as np
import nltk
import networkx as nx
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from rouge_score import rouge_scorer
import string
import torch
from transformers import PegasusTokenizer, PegasusForConditionalGeneration

nltk.download('punkt_tab')
nltk.download('stopwords')

# Step 2: Load test data
test_df = pd.read_excel("test_data.xlsx",)
texts = test_df["Text"].tolist()
references = test_df["Summary"].tolist()

# Step 3: Define TextRank functions
stop_words = set(stopwords.words('english'))

def preprocess_sentence(sentence):
    words = word_tokenize(sentence.lower())
    return [w for w in words if w not in stop_words and w not in string.punctuation]

def textrank_rank_sentences(text):
    sentences = sent_tokenize(text)
    if len(sentences) <= 1:
        return sentences
    preprocessed = [preprocess_sentence(s) for s in sentences]

    similarity_matrix = np.zeros((len(sentences), len(sentences)))
    for i in range(len(sentences)):
        for j in range(len(sentences)):
            if i == j:
                continue
            words_i = set(preprocessed[i])
            words_j = set(preprocessed[j])
            if not words_i or not words_j:
                continue
            common_words = words_i.intersection(words_j)
            if common_words:
                similarity_matrix[i][j] = len(common_words) / (np.log(len(words_i)+1) + np.log(len(words_j)+1))

    graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(graph)
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    return [s for _, s in ranked_sentences]

# Step 4: Load Legal-Pegasus
model_name = "nsi319/legal-pegasus"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

def generate_pegasus_summary(text, max_length=128, min_length=30):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="longest", max_length=512).to(device)
    summary_ids = model.generate(inputs["input_ids"], max_length=max_length, min_length=min_length, length_penalty=1.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Step 5: Hybrid Summarization (TextRank + Pegasus)
generated_summaries = []
for text in texts:
    ranked_sentences = textrank_rank_sentences(text)
    top_sentences = " ".join(ranked_sentences[:10])  # select top 10 sentences
    summary = generate_pegasus_summary(top_sentences)
    generated_summaries.append(summary)

# Step 6: ROUGE Evaluation
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
rouge_scores = {"rouge1": [], "rouge2": [], "rougeL": []}

for ref, gen in zip(references, generated_summaries):
    score = scorer.score(ref, gen)
    for key in rouge_scores:
        rouge_scores[key].append(score[key].fmeasure)

average_scores = {key: np.mean(vals) for key, vals in rouge_scores.items()}
print("ROUGE Scores:")
for key, val in average_scores.items():
    print(f"{key}: {val:.4f}")